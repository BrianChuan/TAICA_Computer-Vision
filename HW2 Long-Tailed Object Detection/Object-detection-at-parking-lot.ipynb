{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyME2doL5xoGNZt4yo59FYuJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrianChuan/TAICA_Computer-Vision/blob/main/HW2_Parking_Lot_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 匯入必要套件＆定義資料路徑\n",
        "> import Libibraies & Define Data Path\n",
        "- 模型：Faster R-CNN model\n",
        "- 任務：多類別物件偵測 (停車場)"
      ],
      "metadata": {
        "id": "aQ-W5dcHCsE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "# ------------------------------\n",
        "# 1. 資料集路徑 (Kaggle 環境)\n",
        "# ------------------------------\n",
        "BASE_PATH = '/kaggle/input/parking-lot/CVPDL_hw2/CVPDL_hw2/'\n",
        "TRAIN_IMG_DIR = os.path.join(BASE_PATH, 'train/img')\n",
        "TRAIN_TXT_DIR = os.path.join(BASE_PATH, 'train/txt') # 標註檔資料夾\n",
        "TEST_IMG_DIR  = os.path.join(BASE_PATH, 'test/img')\n",
        "\n",
        "# ------------------------------\n",
        "# 2. 類別定義\n",
        "# ------------------------------\n",
        "# 檔案中的 class id: 0: car, 1: hov, 2: person, 3: motorcycle\n",
        "# 模型中的 class id: 1: car, 2: hov, 3: person, 4: motorcycle (0 保留給背景)\n",
        "CLASSES = ['car', 'hov', 'person', 'motorcycle']\n",
        "NUM_CLASSES = len(CLASSES) + 1 # +1 for background\n",
        "\n",
        "print(f\"✅ Base path: {BASE_PATH}\")\n",
        "print(f\"✅ Number of classes: {NUM_CLASSES}\")"
      ],
      "metadata": {
        "id": "ibBea7p2LZME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations -q"
      ],
      "metadata": {
        "id": "ZmzawVR-DwY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset 實作\n",
        "- 功能：讀取圖片與對應的 `.txt` 標註檔案，並轉換成訓練用的 Tensor 格式。"
      ],
      "metadata": {
        "id": "kFdD8-NPJ6G4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# 3. Dataset (修改為 ParkingLotDataset)\n",
        "# ------------------------------\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "def get_train_transforms():\n",
        "    return A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.2),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "class ParkingLotDataset(Dataset):\n",
        "    def __init__(self, img_dir, txt_dir, transforms=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.txt_dir = txt_dir\n",
        "        self.transforms = transforms\n",
        "        self.imgs = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.imgs[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # 根據圖片名稱找到對應的 txt 標註檔\n",
        "        txt_name = os.path.splitext(img_name)[0] + '.txt'\n",
        "        txt_path = os.path.join(self.txt_dir, txt_name)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        if os.path.exists(txt_path):\n",
        "            with open(txt_path) as f:\n",
        "                for line in f:\n",
        "                    line = line.strip().split()\n",
        "                    if len(line) < 5:\n",
        "                        continue\n",
        "                    \n",
        "                    class_id = int(line[0])\n",
        "                    x, y, w, h = map(float, line[1:5])\n",
        "                    \n",
        "                    if w <= 0 or h <= 0:\n",
        "                        continue\n",
        "                    \n",
        "                    # 轉換為 [x_min, y_min, x_max, y_max] 格式\n",
        "                    x_min = x\n",
        "                    y_min = y\n",
        "                    x_max = x + w\n",
        "                    y_max = y + h\n",
        "                    \n",
        "                    boxes.append([x_min, y_min, x_max, y_max])\n",
        "                    # 重要：將檔案中的 class_id (0-3) 轉換為模型需要的 id (1-4)\n",
        "                    labels.append(class_id + 1)\n",
        "\n",
        "        # 如果這張圖沒有任何標註，遞迴取下一張\n",
        "        if not boxes:\n",
        "             return self.__getitem__((idx + 1) % len(self.imgs))\n",
        "        \n",
        "        target = {\n",
        "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.int64)\n",
        "        }\n",
        "        \n",
        "        if self.transforms:\n",
        "            transformed = self.transforms(image=img, bboxes=target['boxes'], labels=target['labels'])\n",
        "            img = transformed['image']\n",
        "            \n",
        "            if not transformed['bboxes']:\n",
        "                return self.__getitem__((idx + 1) % len(self.imgs))\n",
        "\n",
        "            target['boxes'] = torch.tensor(transformed['bboxes'], dtype=torch.float32)\n",
        "            target['labels'] = torch.tensor(transformed['labels'], dtype=torch.int64)\n",
        "        \n",
        "        return img, target, img_name\n"
      ],
      "metadata": {
        "id": "C_RfwDaqLico"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 將 Dataset 物件包裝成 DataLoader"
      ],
      "metadata": {
        "id": "dSpMoVLkI6r1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# 4. DataLoader + Validation Split\n",
        "# ------------------------------\n",
        "\n",
        "def get_val_transforms():\n",
        "    return A.Compose([\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "# 建立一個不帶任何 transform 的初始 dataset\n",
        "full_dataset = ParkingLotDataset(TRAIN_IMG_DIR, TRAIN_TXT_DIR, transforms=None)\n",
        "\n",
        "# 按照 80/20 切分訓練集和驗證集\n",
        "n_total = len(full_dataset)\n",
        "n_val = int(0.2 * n_total)\n",
        "n_train = n_total - n_val\n",
        "train_dataset, val_dataset = random_split(full_dataset, [n_train, n_val])\n",
        "\n",
        "# 為切分後的兩個子集分別賦予不同的 transform\n",
        "train_dataset.dataset.transforms = get_train_transforms()\n",
        "val_dataset.dataset.transforms = get_val_transforms()\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = list(filter(lambda x: x is not None, batch))\n",
        "    if not batch: return (torch.empty(0), torch.empty(0))\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)} | Val size: {len(val_dataset)}\")"
      ],
      "metadata": {
        "id": "vRCoANFZLqZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 模型設定"
      ],
      "metadata": {
        "id": "sSrYD29fKW2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# 5. Faster R-CNN + pretrained weights\n",
        "# ------------------------------\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
        "model = fasterrcnn_resnet50_fpn_v2(weights=\"COCO_V1\")\n",
        "\n",
        "# 替換分類頭以符合我們的類別數量 (NUM_CLASSES)\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
        "model.to(device)\n",
        "print(f\"Model moved to {device}\")"
      ],
      "metadata": {
        "id": "tuZ3x_1eLsnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 參數設定"
      ],
      "metadata": {
        "id": "4Tyg9--lL4wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# 6. 訓練設定\n",
        "# ------------------------------\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-4, weight_decay=1e-4)\n",
        "num_epochs = 10\n",
        "\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
        "\n",
        "best_map = -1.0 # 追蹤歷史最高的驗證 mAP\n",
        "MODEL_SAVE_PATH = 'best_parking_lot_model.pth' # Kaggle 輸出路徑"
      ],
      "metadata": {
        "id": "J-zpjOgpLuNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics -q"
      ],
      "metadata": {
        "id": "GCGMpnMBP36h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (訓練＋驗證＋模型儲存)迴圈"
      ],
      "metadata": {
        "id": "l45bnCxjOn1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# 7. 訓練 + 驗證 Loop\n",
        "# ------------------------------\n",
        "from tqdm.notebook import tqdm\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    model.train()\n",
        "    train_loss_sum = 0.0\n",
        "\n",
        "    for imgs, targets, _ in tqdm(train_loader, desc=f\"Training\"):\n",
        "        imgs = [img.to(device) for img in imgs]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(imgs, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss_sum += losses.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_train_loss = train_loss_sum / len(train_loader)\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # Validation\n",
        "    # ------------------------------\n",
        "    metric = MeanAveragePrecision(box_format='xyxy').to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets, _ in tqdm(val_loader, desc=f\"Validating\"):\n",
        "            imgs = [img.to(device) for img in imgs]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            predictions = model(imgs)\n",
        "            metric.update(predictions, targets)\n",
        "\n",
        "    results = metric.compute()\n",
        "    val_map = results['map'].item()\n",
        "    print(f\"Validation mAP: {val_map:.4f}\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # 模型儲存邏輯 (以 mAP 為標準)\n",
        "    # ------------------------------\n",
        "    if val_map > best_map:\n",
        "        print(f\"Validation mAP Improved ({best_map:.4f} -> {val_map:.4f}). Saving model...\")\n",
        "        best_map = val_map\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "    else:\n",
        "        print(f\"Validation mAP did not improve from {best_map:.4f}.\")"
      ],
      "metadata": {
        "id": "L_b_f35DLyC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 預測與提交"
      ],
      "metadata": {
        "id": "pred_md_cell"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# 8. 預測 & submission\n",
        "# ------------------------------\n",
        "# 載入表現最好的模型權重\n",
        "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "model.eval()\n",
        "\n",
        "test_imgs = sorted(os.listdir(TEST_IMG_DIR))\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img_name in tqdm(test_imgs, desc=\"Testing\"):\n",
        "        img_path = os.path.join(TEST_IMG_DIR, img_name)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img_tensor = F.to_tensor(img).to(device)\n",
        "        pred = model([img_tensor])[0]\n",
        "\n",
        "        img_id = int(os.path.splitext(img_name)[0])\n",
        "        parts = []\n",
        "        for score, label, box in zip(pred['scores'], pred['labels'], pred['boxes']):\n",
        "            if score < 0.3: # 可調整信心度閾值\n",
        "                continue\n",
        "            \n",
        "            # 將模型的 label (1-4) 轉回檔案格式的 label (0-3)\n",
        "            class_id = label.item() - 1 \n",
        "\n",
        "            x_min, y_min, x_max, y_max = box.tolist()\n",
        "            w, h = x_max - x_min, y_max - y_min\n",
        "            parts.append(f\"{score:.6f} {x_min:.2f} {y_min:.2f} {w:.2f} {h:.2f} {class_id}\")\n",
        "\n",
        "        pred_str = \" \".join(parts)\n",
        "        predictions.append([img_id, pred_str])\n",
        "\n",
        "submission = pd.DataFrame(predictions, columns=['Image_ID', 'PredictionString'])\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"\\n✅ Submission saved: submission.csv\")"
      ],
      "metadata": {
        "id": "ZpTo5I99E6Td"
      }
    }
  ]
}