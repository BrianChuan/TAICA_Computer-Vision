{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gCC1B-i7Gs0N",
        "QnBVzcixswdE"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJuosWRdBJ4tzJ4XbV8keH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrianChuan/TAICA_Computer-Vision/blob/main/HW1%20Object%20Detection/HW1_%E6%B3%A8%E5%B0%84%E5%A0%B4%E8%B1%AC%E9%9A%BB%E8%BE%A8%E8%AD%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A. 資料匯入與前處理"
      ],
      "metadata": {
        "id": "3DeJ2OE752G1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Colab 雲端硬碟掛載 (Mount Google Drive)"
      ],
      "metadata": {
        "id": "UB8Rwzfh5x-T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxBXVxdK3jdb",
        "outputId": "8851499d-779c-41c6-f751-d46484be530c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 資料路徑設定 (Data Path Setup)"
      ],
      "metadata": {
        "id": "S3m9U9V-6BvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 設定訓練資料集的根目錄\n",
        "DATA_ROOT = '/content/drive/MyDrive/Colab Notebooks/電腦視覺與深度學習/taica-cvpdl-2025-hw-1/train/'\n",
        "\n",
        "# 圖像資料夾路徑\n",
        "IMAGE_DIR = DATA_ROOT + 'img/'\n",
        "\n",
        "# 標註檔案路徑\n",
        "GT_FILE_PATH = DATA_ROOT + 'gt.txt'\n",
        "\n",
        "print(f\"圖像資料夾路徑: {IMAGE_DIR}\")\n",
        "print(f\"標註檔案路徑: {GT_FILE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xdq9eoMF6C-z",
        "outputId": "bda8a1d2-0608-4249-fa88-4496e8f441a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "圖像資料夾路徑: /content/drive/MyDrive/Colab Notebooks/電腦視覺與深度學習/taica-cvpdl-2025-hw-1/train/img/\n",
            "標註檔案路徑: /content/drive/MyDrive/Colab Notebooks/電腦視覺與深度學習/taica-cvpdl-2025-hw-1/train/gt.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gt.txt 資料解析與格式轉換 (Data Parsing)"
      ],
      "metadata": {
        "id": "379z9lGM6xiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 1. 讀取 gt.txt 檔案\n",
        "df = pd.read_csv(\n",
        "    GT_FILE_PATH,\n",
        "    header=None,\n",
        "    names=['frame_id', 'bb_left', 'bb_top', 'bb_width', 'bb_height'],\n",
        "    delimiter=',' # 確保分隔符是逗號\n",
        ")\n",
        "\n",
        "# 2. 轉換 frame_id 到 圖像檔案名 (Image_ID)\n",
        "df['image_filename'] = df['frame_id'].apply(lambda x: f'{x:08d}.jpg')\n",
        "\n",
        "# 3. 計算 x_center 和 y_center (常用於 One-stage 模型)\n",
        "# 原始格式: [x_min, y_min, w, h] (bb_left, bb_top, bb_width, bb_height)\n",
        "# 轉換目標: [x_center, y_center, w, h] (常用於 YOLO 類模型)\n",
        "df['x_center'] = df['bb_left'] + df['bb_width'] / 2\n",
        "df['y_center'] = df['bb_top'] + df['bb_height'] / 2\n",
        "\n",
        "# 整理最終的 DataFrame\n",
        "df_gt = df[['image_filename', 'bb_left', 'bb_top', 'bb_width', 'bb_height', 'x_center', 'y_center']]\n",
        "\n",
        "# 顯示前幾行確認解析正確\n",
        "print(\"\\n--- Ground Truth Data (前 5 行) ---\")\n",
        "print(df_gt.head())\n",
        "print(f\"\\n總標註數量: {len(df_gt)}\")\n",
        "print(f\"總圖像數量 (不重複): {df_gt['image_filename'].nunique()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUvzvCud6y98",
        "outputId": "20ff789b-5ce8-4eb3-8658-e319698c9597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Ground Truth Data (前 5 行) ---\n",
            "  image_filename  bb_left  bb_top  bb_width  bb_height  x_center  y_center\n",
            "0   00000001.jpg      307      50        96         18     355.0      59.0\n",
            "1   00000001.jpg      308      63       101         41     358.5      83.5\n",
            "2   00000001.jpg      330      78       104         76     382.0     116.0\n",
            "3   00000001.jpg      403      47        52         79     429.0      86.5\n",
            "4   00000001.jpg      440      55        59         83     469.5      96.5\n",
            "\n",
            "總標註數量: 38747\n",
            "總圖像數量 (不重複): 1270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B. 資料載入器＆模型建構"
      ],
      "metadata": {
        "id": "3etnPcvS9ILj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch 資料集類別 (Dataset Class)\n",
        "- 將圖像與解析後的 gt.txt 送入模型。\n",
        "- 使用 PyTorch 框架來定義此類別的 Dataset。\n",
        "- 關鍵功能：將像素座標轉換為訓練 One-Stage 模型所需的標準化座標。\n"
      ],
      "metadata": {
        "id": "ni6e0koe9roi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "# 我們需要使用 pandas DataFrame，假設您已經完成了步驟 A 的解析\n",
        "# from B.1:\n",
        "# df_gt = ... (您的 Ground Truth DataFrame)\n",
        "# IMAGE_DIR = ... (圖像資料夾路徑)\n",
        "\n",
        "class PigDetectionDataset(Dataset):\n",
        "    def __init__(self, df_gt, image_dir, image_size=(416, 416), transform=None):\n",
        "        \"\"\"\n",
        "        初始化資料集。\n",
        "        :param df_gt: 包含所有邊界框標註的 Pandas DataFrame。\n",
        "        :param image_dir: 圖像檔案的根目錄。\n",
        "        :param image_size: 訓練時將圖像resize到的目標尺寸 (W, H)。\n",
        "        :param transform: 資料增強 (Data Augmentation) 函數。\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.image_size = image_size\n",
        "        self.transform = transform\n",
        "\n",
        "        # 取得所有不重複的圖像檔案名\n",
        "        self.image_files = df_gt['image_filename'].unique().tolist()\n",
        "\n",
        "        # 將 DataFrame 轉換為以檔案名為鍵的字典，方便快速查找該圖像的所有標註\n",
        "        self.annotations = {\n",
        "            name: df_gt[df_gt['image_filename'] == name].to_dict('records')\n",
        "            for name in self.image_files\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            # 1. 載入圖像\n",
        "            img_filename = self.image_files[idx]\n",
        "            img_path = os.path.join(self.image_dir, img_filename)\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            # 2. 獲取原始圖像尺寸\n",
        "            original_w, original_h = img.size\n",
        "\n",
        "            # 3. 獲取該圖像的所有邊界框 (BB)\n",
        "            boxes_data = self.annotations[img_filename]\n",
        "\n",
        "            # 轉換為 [x_center, y_center, w, h] 格式的 NumPy 陣列\n",
        "            # 並且進行標準化 (Normalization)\n",
        "            # One-stage 模型通常使用 0~1 的相對座標\n",
        "\n",
        "            targets = []\n",
        "            for box in boxes_data:\n",
        "                # 原始像素值\n",
        "                x_min, y_min, w, h = box['bb_left'], box['bb_top'], box['bb_width'], box['bb_height']\n",
        "                x_center = x_min + w / 2\n",
        "                y_center = y_min + h / 2\n",
        "\n",
        "                # 標準化 (除以原始圖像的寬高)\n",
        "                norm_x_center = x_center / original_w\n",
        "                norm_y_center = y_center / original_h\n",
        "                norm_w = w / original_w\n",
        "                norm_h = h / original_h\n",
        "\n",
        "                # 對於單一類別目標偵測（豬），類別 ID 為 0 [cite: 114]\n",
        "                # 訓練資料格式: [class_id, x_center_norm, y_center_norm, w_norm, h_norm]\n",
        "                targets.append([0, norm_x_center, norm_y_center, norm_w, norm_h])\n",
        "\n",
        "            targets = np.array(targets, dtype=np.float32)\n",
        "\n",
        "            # 4. 圖像 Resize (並在 Data Loader 中應用 Transform/Augmentation)\n",
        "            img = img.resize(self.image_size)\n",
        "            img = np.array(img).transpose(2, 0, 1) / 255.0 # HWC -> CWH, 0~1 標準化\n",
        "\n",
        "            # 5. 轉換為 PyTorch Tensor\n",
        "            image_tensor = torch.from_numpy(img).float()\n",
        "            target_tensor = torch.from_numpy(targets).float()\n",
        "\n",
        "            return image_tensor, target_tensor\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            # --- 檔案讀取失敗的處理機制 ---\n",
        "            # print(f\"⚠️ Warning: File not found at index {index}, path: {img_path}. Sampling replacement.\")\n",
        "\n",
        "            # 1. 選擇一個隨機的有效索引作為替代\n",
        "            # 確保不會無限遞歸，如果隨機選到的還是錯的，會再次進入 except 區塊\n",
        "            random_index = random.randint(0, len(self) - 1)\n",
        "\n",
        "            # 2. 遞歸呼叫 __getitem__，直到找到一個有效的資料\n",
        "            # ⚠️ 遞歸呼叫時，將 num_workers 設置為 0 以避免 worker 卡死\n",
        "            # 這是標準且安全的方法，可以確保 worker 不會因為單個錯誤停止\n",
        "            return self.__getitem__(random_index)"
      ],
      "metadata": {
        "id": "zSX5-fff9pp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-Stage 模型核心結構定義 (Model Core)\n",
        "- 開始定義一個 YOLO 模型的核心骨幹和輸出層。\n",
        "- *注意：每個層都比序從零開始初始化。\n",
        "\n",
        "### 1. 骨幹網路(Backbone)\n",
        "> 這裡使用簡化的 Conv/Batch Norm/LeakyReLU 模塊作為基礎"
      ],
      "metadata": {
        "id": "LwT7Hw4d_fA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c, kernel_size, stride, padding):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_c, out_c, kernel_size, stride, padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_c)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.leaky_relu(self.bn(self.conv(x)))\n",
        "\n",
        "# 簡化的骨幹 (例如 ResNet/Darknet-like)\n",
        "class SimpleBackbone(nn.Module):\n",
        "    def __init__(self, in_c=3):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            ConvBlock(in_c, 32, 3, 1, 1),\n",
        "            nn.MaxPool2d(2, 2), # 降採樣\n",
        "            ConvBlock(32, 64, 3, 1, 1),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            ConvBlock(64, 128, 3, 1, 1),\n",
        "            ConvBlock(128, 64, 1, 1, 0),\n",
        "            ConvBlock(64, 128, 3, 1, 1),\n",
        "            # ... 這裡可以繼續添加更多的卷積層和殘差塊來加深網路\n",
        "        )\n",
        "        # 確保所有層次都是從零開始初始化\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # He initialization (適合 ReLU/Leaky ReLU)\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)"
      ],
      "metadata": {
        "id": "c2h1lJNp_5Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 檢測頭(Detection Head)\n",
        "> 負責將骨幹提取的特徵轉換為最終的邊界框預測\n",
        "\n",
        "- 對於YOLO，每個網格預測 $B$ 個錨匡。\n",
        "- 每個錨匡會有：物體信心度、類別信心度、邊界匡座標，三種資訊。\n",
        "- 輸出通道數：$B×(4(coords)+1(confidence)+1(class))=B×6$\n",
        "- 以下實驗中假設 $B=3$"
      ],
      "metadata": {
        "id": "jABEOuESAYNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLOHead(nn.Module):\n",
        "    def __init__(self, in_c, num_anchors=3):\n",
        "        super().__init__()\n",
        "        # 最終輸出層\n",
        "        # B * (4 + 1 + 1) = num_anchors * 6\n",
        "        self.conv = nn.Conv2d(in_c, num_anchors * 6, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 輸出格式: (Batch, num_anchors * 6, Grid_H, Grid_W)\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "k_n8daZcBhor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C. 複雜資料增強\n",
        "> 在 YOLO 系列的訓練中，Mosaic 增強是最具影響力的技術之一。"
      ],
      "metadata": {
        "id": "NX7i42R3FqIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Mosaic 增強原理\n",
        "> 將四張隨機選取的訓練圖像，通過隨機裁剪、縮放、旋轉等方式，拼接成一張新的訓練圖像。\n",
        "- 圖像處理： 四張圖被放置在四個象限，中間的接縫點 (Center Point) 是隨機選取的。\n",
        "- 標註處理： 四張圖的所有真實邊界框都必須根據其在新的 Mosaic 圖像中的位置重新計算座標。"
      ],
      "metadata": {
        "id": "UNrSUXiOGHUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 實作 Mosaic 函數\n"
      ],
      "metadata": {
        "id": "gCC1B-i7Gs0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 # 通常用於資料增強和圖像處理\n",
        "import random\n",
        "import torch\n",
        "\n",
        "# 圖像處理和增強的工具函數 (為了簡潔，這裡僅為示意)\n",
        "# 實際的實作需要包含圖像裁剪、縮放、顏色空間轉換等細節。\n",
        "\n",
        "def get_mosaic_item(dataset, full_mosaic_size=416):\n",
        "    \"\"\"\n",
        "    從訓練數據集中隨機獲取並拼接四張圖像及其標註。\n",
        "\n",
        "    :param dataset: 您的 PigDetectionDataset 實例。\n",
        "    :param full_mosaic_size: 最終 Mosaic 圖像的寬高 (例如 416x416)。\n",
        "    :return: 拼接後的圖像張量 (Tensor) 和新的標註 (Normalized Targets)。\n",
        "    \"\"\"\n",
        "    mosaic_img = np.zeros((full_mosaic_size * 2, full_mosaic_size * 2, 3), dtype=np.uint8)\n",
        "    mosaic_targets = []\n",
        "\n",
        "    # 隨機決定 Mosaic 接縫點的中心 (中心點在 0.5 到 1.5 倍尺寸之間)\n",
        "    # 我們將中心點隨機化到 (0.5*S ~ 1.5*S) 的範圍\n",
        "    s = full_mosaic_size\n",
        "    center_x = int(random.uniform(0.5 * s, 1.5 * s))\n",
        "    center_y = int(random.uniform(0.5 * s, 1.5 * s))\n",
        "\n",
        "    # 四個圖像的索引 (隨機抽取四張圖)\n",
        "    indices = [random.randint(0, len(dataset) - 1) for _ in range(4)]\n",
        "\n",
        "    # 處理四個象限 (i=0, 1, 2, 3)\n",
        "    for i, index in enumerate(indices):\n",
        "        # 1. 獲取單張圖像和標註\n",
        "        # 注意: 這裡我們呼叫 dataset.__getitem__ 來獲取原始圖像和標準化標註\n",
        "        # 為了簡化，我們假設這裡的 dataset.__getitem__ 返回的是 PIL 圖像和未經標準化的像素邊界框\n",
        "        # 在實際的 YOLO 實作中，__getitem__ 應該返回原始圖像和像素 BB，然後在這裡進行隨機縮放和標準化。\n",
        "\n",
        "        # 由於我們沒有完整的Dataset類，這裡暫時將圖像載入和標註解析整合：\n",
        "        # 實際應用中，您會從 dataset[index] 獲取數據。\n",
        "\n",
        "        # 為了讓 Colab 環境可行，這裡只能做基本示意：\n",
        "        # 我們假設從 dataset 獲取的 img_i 是已經 Resize 到 S x S 的 numpy 圖像，\n",
        "        # 且 targets_i 是已經標準化的 [class, x_norm, y_norm, w_norm, h_norm]\n",
        "\n",
        "        # 實際訓練代碼中，您需要從 dataset[index] 獲取未處理的原始圖像和像素座標\n",
        "        # 這裡我們用一個簡易的替代方案 (你需要自行調整 Dataset 類別):\n",
        "\n",
        "        # --- 簡易替代方案開始 (實際訓練需要更複雜的數據管道) ---\n",
        "        img_tensor, target_tensor = dataset[index]\n",
        "        # 將 tensor 轉回 numpy (HWC)\n",
        "        img_i = (img_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "        targets_i = target_tensor.numpy()\n",
        "        h, w, _ = img_i.shape\n",
        "        # --- 簡易替代方案結束 ---\n",
        "\n",
        "        # 2. 確定圖像放置的象限 (top-left, top-right, bottom-left, bottom-right)\n",
        "\n",
        "        if i == 0: # 頂部左側 (Top-Left)\n",
        "            # Mosaic 圖像上的座標範圍 (x_min, y_min) -> (x_max, y_max)\n",
        "            x_min, y_min = center_x - w, center_y - h\n",
        "            x_max, y_max = center_x, center_y\n",
        "\n",
        "        elif i == 1: # 頂部右側 (Top-Right)\n",
        "            x_min, y_min = center_x, center_y - h\n",
        "            x_max, y_max = center_x + w, center_y\n",
        "\n",
        "        elif i == 2: # 底部左側 (Bottom-Left)\n",
        "            x_min, y_min = center_x - w, center_y\n",
        "            x_max, y_max = center_x, center_y + h\n",
        "\n",
        "        elif i == 3: # 底部右側 (Bottom-Right)\n",
        "            x_min, y_min = center_x, center_y\n",
        "            x_max, y_max = center_x + w, center_y + h\n",
        "\n",
        "        # 3. 裁剪與貼合 (Crop and Paste)\n",
        "\n",
        "        # 確保圖像不會超出 Mosaic 邊界 (防止越界)\n",
        "        x_min = max(x_min, 0)\n",
        "        y_min = max(y_min, 0)\n",
        "        x_max = min(x_max, full_mosaic_size * 2)\n",
        "        y_max = min(y_max, full_mosaic_size * 2)\n",
        "\n",
        "        # 貼上圖像\n",
        "        # (這裡需要更複雜的裁剪邏輯來處理圖像和邊界框的邊界情況)\n",
        "\n",
        "        # 簡化貼圖（假設不考慮裁剪，僅做貼上）\n",
        "        if x_max > x_min and y_max > y_min:\n",
        "             mosaic_img[y_min:y_max, x_min:x_max] = img_i[:y_max-y_min, :x_max-x_min]\n",
        "\n",
        "        # 4. 邊界框座標轉換\n",
        "        # 將邊界框從原本圖像的標準化座標，轉換到 Mosaic 圖像的絕對像素座標\n",
        "        for target in targets_i:\n",
        "            c, x_norm, y_norm, w_norm, h_norm = target\n",
        "\n",
        "            # 轉換回原始圖像的像素座標 (假設 img_i 是 S x S)\n",
        "            # x_pixel = x_norm * w (S)\n",
        "            # y_pixel = y_norm * h (S)\n",
        "\n",
        "            # --- 簡化標註轉換開始 ---\n",
        "            # 為了實作，您需要將 target_tensor 中的標準化座標先轉換回像素座標\n",
        "            # 然後加上 (x_min, y_min) 的偏移量，再進行裁剪處理\n",
        "            # 並最終將其標準化到 (0, 0) -> (2S, 2S) 的範圍\n",
        "\n",
        "            # 以下是將標準化座標轉換為 Mosaic 座標的核心邏輯 (你需要自己完成像素轉換的細節)\n",
        "\n",
        "            # new_x_center = (x_norm * w) + x_min\n",
        "            # new_y_center = (y_norm * h) + y_min\n",
        "            # new_w = w_norm * w\n",
        "            # new_h = h_norm * h\n",
        "\n",
        "            # --- 簡化標註轉換結束 ---\n",
        "\n",
        "            # 假設已經完成了座標轉換和裁剪\n",
        "            # final_target = [c, new_x_center, new_y_center, new_w, new_h]\n",
        "            # mosaic_targets.append(final_target)\n",
        "            pass\n",
        "\n",
        "    # 5. 最終處理：將 2S x 2S 的 Mosaic 圖像 Resize/裁剪回 S x S (例如 416x416)\n",
        "    # 這一步通常是隨機裁剪，並將最終的標註標準化到 S x S 範圍\n",
        "\n",
        "    # 這是 Mosaic 實作中最複雜的環節，確保邊界框轉換和裁剪正確是關鍵。\n",
        "\n",
        "    # 最終返回 (經過 HWC -> CWH 轉換和 0~1 標準化的 Tensor):\n",
        "    # final_img_tensor = ...\n",
        "    # final_targets_tensor = ...\n",
        "    # return final_img_tensor, final_targets_tensor\n",
        "\n",
        "    return None, None # 暫時返回 None"
      ],
      "metadata": {
        "id": "XqksDEnNGwGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# D. 錨框 K-means 聚類分析實作"
      ],
      "metadata": {
        "id": "VVpfaugdEH_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 準備程式碼環境與 IoU 距離函數\n",
        "- 定義一個特殊的距離函數，用於 K-means 聚類"
      ],
      "metadata": {
        "id": "HcbS1YGrEU_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "import torch # 假設 df_gt 已經在內存中\n",
        "\n",
        "# 確保我們有 df_gt:\n",
        "# 假設這是從步驟 A 傳遞過來的 DataFrame\n",
        "# df_gt = pd.read_csv(...)\n",
        "\n",
        "def iou_metric(boxes, clusters):\n",
        "    \"\"\"\n",
        "    計算每個邊界框 (boxes) 與每個錨框 (clusters) 之間的 IoU (Intersection over Union)。\n",
        "\n",
        "    :param boxes: 真實邊界框的寬高 [N, 2] (width, height)\n",
        "    :param clusters: 錨框的寬高 [K, 2] (width, height)\n",
        "    :return: IoU 值 [N, K]\n",
        "    \"\"\"\n",
        "    # 假設所有框的中心點都在 (0, 0)\n",
        "    # IoU = 交集面積 / 聯集面積\n",
        "\n",
        "    # 擴展維度以便廣播運算\n",
        "    box_w = np.expand_dims(boxes[:, 0], axis=1) # [N, 1]\n",
        "    box_h = np.expand_dims(boxes[:, 1], axis=1) # [N, 1]\n",
        "    cluster_w = np.expand_dims(clusters[:, 0], axis=0) # [1, K]\n",
        "    cluster_h = np.expand_dims(clusters[:, 1], axis=0) # [1, K]\n",
        "\n",
        "    # 交集 (Intersection) 面積\n",
        "    # 由於中心點對齊，交集面積 = min(w_box, w_cluster) * min(h_box, h_cluster)\n",
        "    inter_w = np.minimum(box_w, cluster_w)\n",
        "    inter_h = np.minimum(box_h, cluster_h)\n",
        "    intersection = inter_w * inter_h # [N, K]\n",
        "\n",
        "    # 聯集 (Union) 面積\n",
        "    box_area = box_w * box_h # [N, 1]\n",
        "    cluster_area = cluster_w * cluster_h # [1, K]\n",
        "    union = box_area + cluster_area - intersection # [N, K]\n",
        "\n",
        "    # 確保不會除以零\n",
        "    iou = np.where(union != 0, intersection / union, 0)\n",
        "    return iou\n",
        "\n",
        "def iou_distance(boxes, clusters):\n",
        "    \"\"\"\n",
        "    定義 K-means 聚類使用的距離度量: 1 - IoU。\n",
        "    距離越小 (IoU 越大) 代表匹配越好。\n",
        "    \"\"\"\n",
        "    iou = iou_metric(boxes, clusters)\n",
        "    # 我們需要找到最大 IoU 的錨框，所以距離是 1 - max(IoU)\n",
        "    max_iou = np.max(iou, axis=1)\n",
        "    # K-means 最小化的是距離的平方和，所以我們回傳 1 - IoU\n",
        "    return 1 - max_iou"
      ],
      "metadata": {
        "id": "HJ54rsgVEfSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 執行 K-means 聚類分析\n",
        "- 通常 YOLOv3 或 YOLOv4 會使用 K=9 或 K=12 個錨框來處理不同尺度的目標。\n",
        "- 本次從 K=9 開始。"
      ],
      "metadata": {
        "id": "c652Qb8SEvUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 從 DataFrame 中提取所有真實邊界框的寬度和高度\n",
        "# 這裡使用像素值 (bb_width, bb_height)\n",
        "# 注意：這些是原始的像素座標，不是標準化後的 (0~1) 值\n",
        "data = df_gt[['bb_width', 'bb_height']].values\n",
        "\n",
        "# 選擇錨框的數量 (K)\n",
        "NUM_ANCHORS = 9\n",
        "\n",
        "# 初始化 K-means 模型\n",
        "# 我們使用標準的 sklearn.cluster.KMeans，但需要手動實作自定義的距離計算\n",
        "# 因為 sklearn 不支持直接使用自定義距離進行聚類，我們模擬其行為\n",
        "# 實際的優化器需要手動實現 IoU K-means，這裡我們使用一個簡化/近似方法\n",
        "# 注意: 這裡的 K-means 實作並不完全等同於 YOLO 中使用的 IoU K-means\n",
        "# 但可以作為一個優秀的起點。\n",
        "kmeans = KMeans(n_clusters=NUM_ANCHORS,\n",
        "                random_state=42,\n",
        "                n_init=10,\n",
        "                max_iter=500).fit(data)\n",
        "\n",
        "# 獲取聚類中心，即錨框的寬高（像素值）\n",
        "anchor_boxes_pixel = kmeans.cluster_centers_\n",
        "\n",
        "# 對錨框進行排序 (例如按面積大小)\n",
        "anchor_boxes_pixel = anchor_boxes_pixel[\n",
        "    np.argsort(anchor_boxes_pixel[:, 0] * anchor_boxes_pixel[:, 1])\n",
        "]\n",
        "\n",
        "# 計算平均 IoU (Average IoU) 作為聚類質量的指標\n",
        "avg_iou = np.mean(iou_distance(data, anchor_boxes_pixel))\n",
        "mean_avg_iou = 1 - avg_iou\n",
        "\n",
        "\n",
        "print(f\"--- K-means ({NUM_ANCHORS} 個錨框) 聚類結果 ---\")\n",
        "print(f\"平均 IoU (Mean Avg IoU) 為: {mean_avg_iou:.4f}\")\n",
        "print(\"\\n最終錨框尺寸 (像素 W, H) - 由小到大排序:\")\n",
        "for i, (w, h) in enumerate(anchor_boxes_pixel):\n",
        "    print(f\"Anchor {i+1}: 寬={w:.2f}, 高={h:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtq5bUH4Ex2S",
        "outputId": "cbb58924-97a0-4c92-ee8c-fadbed6e8807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- K-means (9 個錨框) 聚類結果 ---\n",
            "平均 IoU (Mean Avg IoU) 為: 0.7522\n",
            "\n",
            "最終錨框尺寸 (像素 W, H) - 由小到大排序:\n",
            "Anchor 1: 寬=28.62, 高=28.98\n",
            "Anchor 2: 寬=35.99, 高=52.87\n",
            "Anchor 3: 寬=59.45, 高=40.71\n",
            "Anchor 4: 寬=92.40, 高=41.99\n",
            "Anchor 5: 寬=55.08, 高=84.09\n",
            "Anchor 6: 寬=129.55, 高=55.87\n",
            "Anchor 7: 寬=89.84, 高=81.11\n",
            "Anchor 8: 寬=81.40, 高=127.60\n",
            "Anchor 9: 寬=135.95, 高=107.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO 複合損失函數製作\n",
        "\n",
        "主要分類：\n",
        "- 邊界框定位損失 (Coord Loss)\n",
        "- 物體信心度損失 (Objectness Loss)\n",
        "- 分類損失 (Classification Loss)"
      ],
      "metadata": {
        "id": "KcSJ0iIDG8NG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 預先定義"
      ],
      "metadata": {
        "id": "WfFkawoRo9or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    計算兩個邊界框之間的 IoU (Intersection over Union)。\n",
        "\n",
        "    :param box1: [..., 4] (x_min, y_min, x_max, y_max)\n",
        "    :param box2: [..., 4] (x_min, y_min, x_max, y_max)\n",
        "    :return: [..., 1] 的 IoU 張量\n",
        "    \"\"\"\n",
        "    # 確保所有維度匹配，進行廣播運算\n",
        "\n",
        "    # 獲取交集區域的座標\n",
        "    x1 = torch.max(box1[..., 0], box2[..., 0])\n",
        "    y1 = torch.max(box1[..., 1], box2[..., 1])\n",
        "    x2 = torch.min(box1[..., 2], box2[..., 2])\n",
        "    y2 = torch.min(box1[..., 3], box2[..., 3])\n",
        "\n",
        "    # 計算交集面積 (確保面積非負)\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "\n",
        "    # 計算各自面積\n",
        "    area1 = (box1[..., 2] - box1[..., 0]) * (box1[..., 3] - box1[..., 1])\n",
        "    area2 = (box2[..., 2] - box2[..., 0]) * (box2[..., 3] - box2[..., 1])\n",
        "\n",
        "    # 計算聯集面積\n",
        "    union = area1 + area2 - intersection\n",
        "\n",
        "    # 防止除以零\n",
        "    iou = intersection / (union + 1e-6)\n",
        "    return iou.unsqueeze(-1)"
      ],
      "metadata": {
        "id": "1d5vcoCbpAwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def diou_loss(pred_boxes, target_boxes):\n",
        "    \"\"\"\n",
        "    計算 D-IoU (Distance-IoU) 損失。\n",
        "\n",
        "    :param pred_boxes: 預測框 [..., 4] (x_min, y_min, x_max, y_max)\n",
        "    :param target_boxes: 真實框 [..., 4] (x_min, y_min, x_max, y_max)\n",
        "    \"\"\"\n",
        "    # 1. IoU\n",
        "    iou = calculate_iou(pred_boxes, target_boxes)\n",
        "\n",
        "    # 2. 獲取中心點\n",
        "    pred_c = (pred_boxes[..., 0:2] + pred_boxes[..., 2:4]) / 2\n",
        "    target_c = (target_boxes[..., 0:2] + target_boxes[..., 2:4]) / 2\n",
        "\n",
        "    # 3. 中心點距離平方 (d^2)\n",
        "    center_dist_sq = torch.sum(torch.pow(pred_c - target_c, 2), dim=-1) # [..., 1]\n",
        "\n",
        "    # 4. 最小閉合框 (C)\n",
        "    c_min = torch.min(pred_boxes[..., 0:2], target_boxes[..., 0:2])\n",
        "    c_max = torch.max(pred_boxes[..., 2:4], target_boxes[..., 2:4])\n",
        "\n",
        "    # C 的對角線距離平方 (c^2)\n",
        "    c_dist_sq = torch.sum(torch.pow(c_max - c_min, 2), dim=-1) + 1e-6\n",
        "\n",
        "    # 5. D-IoU 損失\n",
        "    diou_term = center_dist_sq / c_dist_sq\n",
        "    diou = iou - diou_term.unsqueeze(-1) # [..., 1]\n",
        "\n",
        "    # Loss = 1 - D-IoU\n",
        "    loss = 1.0 - diou\n",
        "    return loss"
      ],
      "metadata": {
        "id": "SVZMvhZnpD8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 核心結構"
      ],
      "metadata": {
        "id": "fJigQX86TM7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ⚠️ 確保 calculate_iou 和 diou_loss 已經定義在 Notebook 中\n",
        "# (假設這兩個函數可以正確處理 [..., 4] 的輸入並輸出 [..., 1] 的損失)\n",
        "\n",
        "class YoloLoss(nn.Module):\n",
        "    def __init__(self, anchors, num_classes=1, img_size=416, iou_threshold=0.5):\n",
        "        super().__init__()\n",
        "        self.anchors = anchors # [K, 2] Normalized tensor\n",
        "        self.num_classes = num_classes\n",
        "        self.img_size = img_size\n",
        "        self.iou_threshold = iou_threshold\n",
        "\n",
        "        self.lambda_coord = 5.0\n",
        "        self.lambda_noobj = 0.5\n",
        "        self.lambda_obj = 1.0\n",
        "\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        batch_size, num_anchors_x_6, grid_h, grid_w = predictions.shape\n",
        "        num_anchors = len(self.anchors)\n",
        "\n",
        "        # 調整為 (Batch, Anchors, Grid_H, Grid_W, 6)\n",
        "        predictions = predictions.view(batch_size, num_anchors, 6, grid_h, grid_w).permute(0, 1, 3, 4, 2).contiguous()\n",
        "\n",
        "        # 提取與轉換預測值 (解碼)\n",
        "        pred_xy = torch.sigmoid(predictions[..., 0:2])\n",
        "        pred_wh = torch.exp(predictions[..., 2:4])\n",
        "\n",
        "        # 創建網格座標 (Grid Coordinates)\n",
        "        grid_x = torch.arange(grid_w, device=predictions.device).repeat(grid_h, 1).view([1, 1, grid_h, grid_w]).type_as(predictions)\n",
        "        grid_y = torch.arange(grid_h, device=predictions.device).repeat(grid_w, 1).t().view([1, 1, grid_h, grid_w]).type_as(predictions)\n",
        "\n",
        "        # 初始化用於損失計算的掩碼 (Mask)\n",
        "        obj_mask = torch.zeros(batch_size, num_anchors, grid_h, grid_w, device=predictions.device, requires_grad=False).type_as(predictions)\n",
        "        noobj_mask = torch.ones(batch_size, num_anchors, grid_h, grid_w, device=predictions.device, requires_grad=False).type_as(predictions)\n",
        "        coord_mask = torch.zeros(batch_size, num_anchors, grid_h, grid_w, device=predictions.device, requires_grad=False).type_as(predictions)\n",
        "\n",
        "        target_xy = torch.zeros(batch_size, num_anchors, grid_h, grid_w, 2, device=predictions.device, requires_grad=False).type_as(predictions)\n",
        "        target_wh = torch.zeros(batch_size, num_anchors, grid_h, grid_w, 2, device=predictions.device, requires_grad=False).type_as(predictions)\n",
        "\n",
        "        # --- 關鍵：將真實標註 (targets) 映射到預測網格 (Mapping) ---\n",
        "        for b in range(batch_size):\n",
        "            targets_b = targets[targets[:, 0] == b, 1:]\n",
        "            if targets_b.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            # 1. 轉換真實框到網格座標 [0, G]\n",
        "            target_gs = targets_b.clone()\n",
        "            target_gs[:, 0] *= grid_w\n",
        "            target_gs[:, 1] *= grid_h\n",
        "            target_gs[:, 2] *= grid_w\n",
        "            target_gs[:, 3] *= grid_h\n",
        "\n",
        "            target_cx = target_gs[:, 0]\n",
        "            target_cy = target_gs[:, 1]\n",
        "\n",
        "            target_gx = target_cx.long()\n",
        "            target_gy = target_cy.long()\n",
        "\n",
        "            # 2. 準備 IoU 廣播計算\n",
        "            targets_b_wh = targets_b[:, 2:4] * self.img_size # 像素尺寸 [N_targets, 2]\n",
        "\n",
        "            # 2.1. GT Boxes: [N, 1, 4] 廣播形狀\n",
        "            gt_wh = targets_b_wh.unsqueeze(1)\n",
        "            gt_boxes_for_iou = torch.cat([torch.zeros_like(gt_wh), gt_wh], dim=2)\n",
        "\n",
        "            # 2.2. Anchor Boxes: [1, K, 4] 廣播形狀\n",
        "            anchors_wh_pixels = self.anchors.type_as(targets_b_wh) * self.img_size\n",
        "            anchors_wh_broadcast = anchors_wh_pixels.unsqueeze(0)\n",
        "            anchors_boxes_for_iou = torch.cat([torch.zeros_like(anchors_wh_broadcast), anchors_wh_broadcast], dim=2)\n",
        "\n",
        "            # 3. 計算 IoU: [N, K, 1] - 這裡不會報錯\n",
        "            anchor_ious = calculate_iou(\n",
        "                gt_boxes_for_iou,\n",
        "                anchors_boxes_for_iou\n",
        "            )\n",
        "\n",
        "            best_anchor_ious, best_anchor_idx = anchor_ious.max(dim=1)\n",
        "\n",
        "            # --- 設置 Mask 和 Target ---\n",
        "            for t in range(targets_b.shape[0]):\n",
        "                a = best_anchor_idx[t]\n",
        "                gx, gy = target_gx[t], target_gy[t]\n",
        "\n",
        "                if gx >= grid_w or gy >= grid_h:\n",
        "                    continue\n",
        "\n",
        "                obj_mask[b, a, gy, gx] = 1.0\n",
        "                noobj_mask[b, a, gy, gx] = 0.0\n",
        "                coord_mask[b, a, gy, gx] = 1.0\n",
        "\n",
        "                # 將 Python tuple 轉換為 Tensor 進行賦值\n",
        "                target_xy[b, a, gy, gx] = torch.tensor(\n",
        "                    [target_cx[t] - gx, target_cy[t] - gy],\n",
        "                    device=target_xy.device,\n",
        "                    dtype=target_xy.dtype\n",
        "                )\n",
        "\n",
        "                # 將 Python tuple 轉換為 Tensor 進行賦值\n",
        "                target_wh[b, a, gy, gx] = torch.tensor(\n",
        "                    [target_gs[t, 2], target_gs[t, 3]],\n",
        "                    device=target_wh.device,\n",
        "                    dtype=target_wh.dtype\n",
        "                )\n",
        "\n",
        "        # --- 損失計算 ---\n",
        "\n",
        "        # 1. 座標損失 (D-IoU Loss)\n",
        "\n",
        "        # 獲取預測框的寬高 (轉換為網格座標)\n",
        "        anchors_grid_w = self.anchors.type_as(pred_wh)[..., 0].view(1, num_anchors, 1, 1) * grid_w\n",
        "        anchors_grid_h = self.anchors.type_as(pred_wh)[..., 1].view(1, num_anchors, 1, 1) * grid_h\n",
        "\n",
        "        pred_bw = pred_wh[..., 0] * anchors_grid_w\n",
        "        pred_bh = pred_wh[..., 1] * anchors_grid_h\n",
        "\n",
        "        # 預測框的 (x_min, y_min, x_max, y_max) 網格座標\n",
        "        pred_x_c = pred_xy[..., 0] + grid_x\n",
        "        pred_y_c = pred_xy[..., 1] + grid_y\n",
        "\n",
        "        pred_x1 = pred_x_c - pred_bw / 2\n",
        "        pred_y1 = pred_y_c - pred_bh / 2\n",
        "        pred_x2 = pred_x_c + pred_bw / 2\n",
        "        pred_y2 = pred_y_c + pred_bh / 2\n",
        "        pred_bbox = torch.stack((pred_x1, pred_y1, pred_x2, pred_y2), dim=-1) # [B, A, G, G, 4]\n",
        "\n",
        "        # 真實框的 (x_min, y_min, x_max, y_max) 網格座標\n",
        "        target_x_c = target_xy[..., 0] + grid_x\n",
        "        target_y_c = target_xy[..., 1] + grid_y\n",
        "\n",
        "        target_x1 = target_x_c - target_wh[..., 0] / 2\n",
        "        target_y1 = target_y_c - target_wh[..., 1] / 2\n",
        "        target_x2 = target_x_c + target_wh[..., 0] / 2\n",
        "        target_y2 = target_y_c + target_wh[..., 1] / 2\n",
        "        target_bbox = torch.stack((target_x1, target_y1, target_x2, target_y2), dim=-1) # [B, A, G, G, 4]\n",
        "\n",
        "        # 計算 D-IoU 損失\n",
        "        coord_loss_all = diou_loss(pred_bbox, target_bbox)\n",
        "        coord_loss = self.lambda_coord * (coord_loss_all * coord_mask.unsqueeze(-1)).sum()\n",
        "\n",
        "        # 2. 物體信心度損失 (Objectness Loss)\n",
        "        obj_loss = self.lambda_obj * (self.bce_loss(predictions[..., 4], obj_mask) * obj_mask).sum()\n",
        "        noobj_loss = self.lambda_noobj * (self.bce_loss(predictions[..., 4], torch.zeros_like(obj_mask)) * noobj_mask).sum()\n",
        "\n",
        "        # 3. 分類損失 (Classification Loss)\n",
        "        total_cls_loss = (self.bce_loss(predictions[..., 5], obj_mask) * obj_mask).sum()\n",
        "\n",
        "        # 4. 總損失 (Total Loss)\n",
        "        total_loss = coord_loss + obj_loss + noobj_loss + total_cls_loss\n",
        "\n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "mnOyRA7DHEPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 關鍵函數：D/C-IoU 損失實作"
      ],
      "metadata": {
        "id": "jb8DRSX0oN2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def giou_loss(pred_boxes, target_boxes):\n",
        "    \"\"\"\n",
        "    計算 D-IoU 或 C-IoU 損失。\n",
        "    這裡使用簡化版的 IoU/D-IoU 實現。\n",
        "    \"\"\"\n",
        "    # 假設 pred_boxes 和 target_boxes 都是 [..., 4] 的張量 (x_min, y_min, x_max, y_max)\n",
        "\n",
        "    # 1. 計算 IoU (Intersection over Union)\n",
        "    # ... (需要複雜的張量運算來計算交集和聯集)\n",
        "    iou = calculate_iou(pred_boxes, target_boxes)\n",
        "\n",
        "    # 2. 計算中心點距離平方 (d^2)\n",
        "    # 獲取中心點 (x_center, y_center)\n",
        "    # pred_c = (pred_boxes[..., 0:2] + pred_boxes[..., 2:4]) / 2\n",
        "    # target_c = (target_boxes[..., 0:2] + target_boxes[..., 2:4]) / 2\n",
        "    # center_dist_sq = torch.sum(torch.pow(pred_c - target_c, 2), dim=-1)\n",
        "\n",
        "    # 3. 計算最小閉合框對角線距離平方 (c^2)\n",
        "    # c_min = torch.min(pred_boxes[..., 0:2], target_boxes[..., 0:2])\n",
        "    # c_max = torch.max(pred_boxes[..., 2:4], target_boxes[..., 2:4])\n",
        "    # c_dist_sq = torch.sum(torch.pow(c_max - c_min, 2), dim=-1)\n",
        "\n",
        "    # 4. D-IoU 損失\n",
        "    # diou = iou - (center_dist_sq / c_dist_sq)\n",
        "    # loss = 1 - diou\n",
        "\n",
        "    return 1 - iou.mean() # 這裡使用 IoU 替代，但在實戰中必須使用 D/C-IoU"
      ],
      "metadata": {
        "id": "6FwXj9zYoRDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 模型實例化"
      ],
      "metadata": {
        "id": "dA1DaSSsrK8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 完整的模型架構定義(YoloModel)\n",
        "> 創建一個頂級的`YoloModel`類別，將骨幹網路的特徵圖，傳遞給檢測頭。"
      ],
      "metadata": {
        "id": "ZrIpn7DWre3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# --- 前置模組 (假設 ConvBlock 已經在您的 Notebook 中定義) ---\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c, kernel_size, stride, padding):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_c, out_c, kernel_size, stride, padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_c)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.leaky_relu(self.bn(self.conv(x)))\n",
        "\n",
        "# --- 骨幹網路 (假設 SimpleBackbone 已經在您的 Notebook 中定義) ---\n",
        "class SimpleBackbone(nn.Module):\n",
        "    def __init__(self, in_c=3):\n",
        "        super().__init__()\n",
        "        # 這裡的 features 需要確保輸出通道數為 128，以便傳遞給 YOLOHead\n",
        "        self.features = nn.Sequential(\n",
        "            ConvBlock(in_c, 32, 3, 1, 1), # 3 -> 32\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            ConvBlock(32, 64, 3, 1, 1), # 32 -> 64\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            ConvBlock(64, 128, 3, 1, 1), # 64 -> 128\n",
        "            ConvBlock(128, 64, 1, 1, 0),\n",
        "            ConvBlock(64, 128, 3, 1, 1),\n",
        "            # 確保最終輸出通道為 128\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "# --- 檢測頭 (假設 YOLOHead 已經在您的 Notebook 中定義) ---\n",
        "class YOLOHead(nn.Module):\n",
        "    def __init__(self, in_c, num_anchors=3):\n",
        "        super().__init__()\n",
        "        # 輸出通道: B * (4(coords) + 1(conf) + 1(class))\n",
        "        self.conv = nn.Conv2d(in_c, num_anchors * 6, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "# --- 完整的 YOLO 模型 ---\n",
        "class YoloModel(nn.Module):\n",
        "    def __init__(self, num_anchors=9):\n",
        "        super().__init__()\n",
        "        self.backbone = SimpleBackbone(in_c=3)\n",
        "        # 假設 backbone 的輸出通道是 128\n",
        "        self.head = YOLOHead(in_c=128, num_anchors=num_anchors)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. 特徵提取 (Feature Extraction)\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # 2. 檢測預測 (Detection Prediction)\n",
        "        predictions = self.head(features)\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "3-HTYjJSrqLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 模型實例化與設備設定\n",
        "> 將模型實例化，必且準備在 GPU 上運行。"
      ],
      "metadata": {
        "id": "W-BdKs3Qrt5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 模型參數設定 (這些數值需要根據您的數據集和優化器設定來確定)\n",
        "NUM_ANCHORS = 9 # 從步驟 D 獲得的最佳錨框數量\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "INPUT_SIZE = 416 # 訓練時圖像的輸入尺寸\n",
        "\n",
        "# 2. 模型實例化\n",
        "model = YoloModel(num_anchors=NUM_ANCHORS).to(DEVICE)\n",
        "\n",
        "# 3. 定義損失函數\n",
        "# 假設您從步驟 D 得到的 9 個錨框是 'pixel_anchors' (像素值)\n",
        "# 並且我們將其標準化為相對於輸入尺寸 (416x416) 的值，並轉換為 PyTorch Tensor\n",
        "# 您需要將步驟 D 的輸出替換到這裡\n",
        "pixel_anchors = np.array([\n",
        "    [ 10.00,  13.00], [ 16.00,  30.00], [ 33.00,  23.00], # 假設值\n",
        "    [ 30.00,  61.00], [ 62.00,  45.00], [ 59.00, 119.00],\n",
        "    [116.00,  90.00], [156.00, 198.00], [373.00, 326.00]\n",
        "])\n",
        "\n",
        "# 標準化錨框 (除以輸入尺寸)\n",
        "normalized_anchors = torch.from_numpy(pixel_anchors / INPUT_SIZE).float().to(DEVICE)\n",
        "\n",
        "\n",
        "# 損失函數實例化\n",
        "# 由於 YOLOv3/v4/v5 採用多尺度檢測，這裡的損失函數可能需要處理多個輸出。\n",
        "# 這裡我們使用單一輸出 YOLO 模型的損失函數，這是一個簡化版本。\n",
        "# 注意: 您需要手動將 diou_loss, calculate_iou 等函數定義在您的 notebook 中。\n",
        "criterion = YoloLoss(anchors=normalized_anchors,\n",
        "                     num_classes=1,\n",
        "                     img_size=INPUT_SIZE).to(DEVICE)\n",
        "\n",
        "print(f\"模型已實例化並移動到設備: {DEVICE}\")\n",
        "print(f\"訓練錨框 (標準化): {normalized_anchors.cpu().numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvoGKp-or5Vg",
        "outputId": "0ea32c9f-d297-4cad-d046-51a238d782ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型已實例化並移動到設備: cuda\n",
            "訓練錨框 (標準化): [[0.02403846 0.03125   ]\n",
            " [0.03846154 0.07211538]\n",
            " [0.07932692 0.05528846]\n",
            " [0.07211538 0.14663461]\n",
            " [0.14903846 0.10817308]\n",
            " [0.14182693 0.28605768]\n",
            " [0.27884614 0.21634616]\n",
            " [0.375      0.47596154]\n",
            " [0.89663464 0.78365386]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 優化器設定\n",
        "> 訓練配置的最終步驟：優化器和學習率調度策略。\n"
      ],
      "metadata": {
        "id": "i_iPhKPFoben"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# 訓練超參數\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 300\n",
        "WARMUP_EPOCHS = 3\n",
        "WEIGHT_DECAY = 5e-4 # 權重衰減，防止過擬合\n",
        "\n",
        "# 1. 優化器\n",
        "optimizer = optim.AdamW(model.parameters(),\n",
        "                        lr=LEARNING_RATE,\n",
        "                        weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# 2. 學習率調度器 (在 Warm-up 結束後開始 Cosine Annealing)\n",
        "scheduler = CosineAnnealingLR(optimizer,\n",
        "                              T_max=EPOCHS - WARMUP_EPOCHS,\n",
        "                              eta_min=1e-6)\n",
        "\n",
        "print(f\"優化器: {type(optimizer).__name__}, 初始學習率: {LEARNING_RATE}\")\n",
        "print(f\"學習率調度策略: Cosine Annealing (T_max={EPOCHS - WARMUP_EPOCHS}, eta_min=1e-6)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5TeiQw_ocoL",
        "outputId": "d9158408-c6b1-497c-d36e-216fba077906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "優化器: AdamW, 初始學習率: 0.001\n",
            "學習率調度策略: Cosine Annealing (T_max=297, eta_min=1e-6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 最終訓練迴圈 (The Final Training Loop)"
      ],
      "metadata": {
        "id": "NO9SmRTcse0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Loader Setup"
      ],
      "metadata": {
        "id": "mTRpqwz0slEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 將長 Batch size 轉換乘一個批次張量"
      ],
      "metadata": {
        "id": "pK_w3Pc4uHQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    將批次數據的標註進行填充 (Padding) 或堆疊 (Stack)。\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    targets = []\n",
        "\n",
        "    for img_tensor, target_tensor in batch:\n",
        "        images.append(img_tensor)\n",
        "\n",
        "        # Target 格式: [class_id, x_norm, y_norm, w_norm, h_norm]\n",
        "        # 我們需要新增一個 batch_id 作為第一列\n",
        "        target_with_id = torch.zeros((target_tensor.shape[0], 6), dtype=torch.float32)\n",
        "        target_with_id[:, 0] = len(targets)  # 設置 batch_id\n",
        "        target_with_id[:, 1:] = target_tensor\n",
        "        targets.append(target_with_id)\n",
        "\n",
        "    # 堆疊圖像張量\n",
        "    images = torch.stack(images, 0)\n",
        "    # 將所有 targets 連接成一個大的張量\n",
        "    targets = torch.cat(targets, 0)\n",
        "\n",
        "    return images, targets"
      ],
      "metadata": {
        "id": "CJBevdOSuT_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# 假設 df_gt, IMAGE_DIR, INPUT_SIZE 已經在您的環境中定義\n",
        "# 並且 PigDetectionDataset 類別已經實作\n",
        "\n",
        "# 創建完整的數據集實例\n",
        "full_dataset = PigDetectionDataset(df_gt, IMAGE_DIR, image_size=(INPUT_SIZE, INPUT_SIZE))\n",
        "\n",
        "# 劃分訓練集和驗證集 (例如 90% 訓練, 10% 驗證)\n",
        "train_size = int(0.9 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "BATCH_SIZE = 16 # 請根據您的 GPU 顯存調整 Batch Size\n",
        "\n",
        "# 創建 DataLoader\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2, # 設置工作進程數 (Colab 通常建議 2 或 4)\n",
        "    pin_memory=True,\n",
        "    collate_fn=custom_collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    collate_fn=custom_collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "ax8kJp1YsjDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 訓練單一 Epoch 函數"
      ],
      "metadata": {
        "id": "QnBVzcixswdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import tqdm # 用於顯示進度條\n",
        "\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device, scaler=None):\n",
        "    \"\"\"\n",
        "    執行單一訓練週期的邏輯。\n",
        "    \"\"\"\n",
        "    model.train() # 設置模型為訓練模式\n",
        "    total_loss = 0.0\n",
        "\n",
        "    # 設置進度條\n",
        "    pbar = tqdm.tqdm(dataloader, desc=f\"Epoch Loss: 0.000\", leave=False)\n",
        "\n",
        "    for i, (images, targets) in enumerate(pbar):\n",
        "        # 將數據移動到正確的設備\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # 梯度清零\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # --- 前向傳播 (Forward Pass) ---\n",
        "        # 這裡推薦使用 PyTorch 的 AMP (自動混合精度) 來加速訓練並節省顯存\n",
        "        # with torch.autocast(device_type=device.type if device.type != 'mps' else 'cpu', enabled=device.type != 'cpu'):\n",
        "\n",
        "        predictions = model(images)\n",
        "        loss = criterion(predictions, targets)\n",
        "\n",
        "        # --- 反向傳播與優化 (Backward Pass & Optimization) ---\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # 更新總損失\n",
        "        total_loss += loss.item()\n",
        "        avg_loss = total_loss / (i + 1)\n",
        "\n",
        "        # 更新進度條顯示\n",
        "        pbar.set_description(f\"Epoch Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # 返回平均損失\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "chEHTZzWs0vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 實作 `validate_model` 函數"
      ],
      "metadata": {
        "id": "2FvrF_vH9ikr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def validate_model(model, dataloader, criterion, device):\n",
        "    \"\"\"\n",
        "    執行一個 Epoch 的模型驗證，計算平均損失。\n",
        "    \"\"\"\n",
        "    model.eval()  # 將模型設置為評估模式\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    # 驗證階段不需要計算梯度\n",
        "    with torch.no_grad():\n",
        "        for images, targets in dataloader:\n",
        "            images = images.to(device)\n",
        "            # targets 保持在 CPU 上，直到在損失函數中傳輸到 GPU/Device\n",
        "\n",
        "            # 1. 前向傳播 (Forward Pass)\n",
        "            predictions = model(images)\n",
        "\n",
        "            # 2. 計算損失\n",
        "            # 這裡我們將 targets 傳輸到與 predictions 相同的設備上\n",
        "            targets_device = targets.to(device)\n",
        "            loss = criterion(predictions, targets_device)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    # 計算平均驗證損失\n",
        "    avg_loss = total_loss / num_batches\n",
        "    model.train() # 驗證完成後，將模型切換回訓練模式\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "R9z7tMsA9pxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 主訓練函數"
      ],
      "metadata": {
        "id": "pgW597Kss3DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs):\n",
        "\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    print(f\"\\n--- 開始訓練，總 Epoch 數: {epochs} ---\")\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
        "\n",
        "        # ---------------------\n",
        "        # 訓練階段\n",
        "        # ---------------------\n",
        "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "        # ---------------------\n",
        "        # 學習率調度 (Warm-up 和 Cosine Annealing)\n",
        "        # ---------------------\n",
        "\n",
        "        # 1. Warm-up 邏輯 (前 WARMUP_EPOCHS)\n",
        "        if epoch <= WARMUP_EPOCHS:\n",
        "            # 計算 Warm-up 學習率\n",
        "            lr = LEARNING_RATE * (0.1 + 0.9 * (epoch / WARMUP_EPOCHS))\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "\n",
        "        # 2. Cosine Annealing 邏輯 (Warm-up 結束後)\n",
        "        elif scheduler is not None:\n",
        "            scheduler.step() # 執行調度器\n",
        "\n",
        "        # 記錄當前學習率\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"訓練損失 (Train Loss): {train_loss:.4f} | LR: {current_lr:.6e}\")\n",
        "\n",
        "\n",
        "        # ---------------------\n",
        "        # 驗證階段 (這裡需要計算 mAP，但為簡化先計算 Val Loss)\n",
        "        # ---------------------\n",
        "        val_loss = validate_model(model, val_loader, criterion, device)\n",
        "        print(f\"驗證損失 (Validation Loss): {val_loss:.4f}\")\n",
        "\n",
        "\n",
        "        # ---------------------\n",
        "        # 模型儲存\n",
        "        # ---------------------\n",
        "        if val_loss < best_loss: # 這裡應該用 mAP 替代 val_loss\n",
        "             best_loss = val_loss\n",
        "             torch.save(model.state_dict(), 'best_yolo_model.pth')\n",
        "             print(\" -> 模型已儲存 (基於 Val Loss)\")\n",
        "\n",
        "    print(\"\\n--- 訓練完成 ---\")"
      ],
      "metadata": {
        "id": "eKNmpTNqs7SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 開始訓練"
      ],
      "metadata": {
        "id": "2rczVwpou7XR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 確保所有超參數已定義 ---\n",
        "\n",
        "# 訓練超參數 (請根據您的實驗需求調整)\n",
        "EPOCHS = 300       # 總訓練週期數\n",
        "BATCH_SIZE = 16    # 批次大小 (請務必根據您的 GPU 顯存調整)\n",
        "LEARNING_RATE = 1e-3\n",
        "WARMUP_EPOCHS = 3\n",
        "\n",
        "# 設備\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- 確保您的所有實例已經準備好 ---\n",
        "# 假設 model, train_loader, val_loader, criterion, optimizer, scheduler, DEVICE 已經被實例化並配置完成\n",
        "\n",
        "print(f\"=========================================\")\n",
        "print(f\"     🚀 開始訓練 (Training Start) 🚀    \")\n",
        "print(f\"=========================================\")\n",
        "print(f\"  目標設備: {DEVICE}\")\n",
        "print(f\"  總 Epochs: {EPOCHS}, Warmup Epochs: {WARMUP_EPOCHS}\")\n",
        "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  訓練集大小: {len(train_loader.dataset)} 圖像\")\n",
        "# print(f\"  驗證集大小: {len(val_loader.dataset)} 圖像\") # 如果您實作了驗證集\n",
        "print(f\"=========================================\")\n",
        "\n",
        "# 呼叫主訓練函數\n",
        "train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader, # 傳入驗證載入器 (如果已實作)\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    device=DEVICE,\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "# 訓練結束後，您可以加載最佳模型權重\n",
        "# best_model_path = 'best_yolo_model.pth'\n",
        "# if os.path.exists(best_model_path):\n",
        "#     model.load_state_dict(torch.load(best_model_path))\n",
        "#     print(f\"已載入最佳模型權重: {best_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "Jg9cWw-ou-4i",
        "outputId": "a10da7ed-e37b-4694-9253-c58f5abe97a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================\n",
            "     🚀 開始訓練 (Training Start) 🚀    \n",
            "=========================================\n",
            "  目標設備: cuda\n",
            "  總 Epochs: 300, Warmup Epochs: 3\n",
            "  Batch Size: 16\n",
            "  訓練集大小: 1143 圖像\n",
            "=========================================\n",
            "\n",
            "--- 開始訓練，總 Epoch 數: 300 ---\n",
            "\n",
            "Epoch 1/300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-291733066.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# 呼叫主訓練函數\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2869748326.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# 訓練階段\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# ---------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# ---------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2744860563.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device, scaler)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch Loss: 0.000\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# 將數據移動到正確的設備\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 準備預測與 NMS"
      ],
      "metadata": {
        "id": "rZjmfhYEv1Pi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 載入 `.pth` 檔案\n",
        "> 回覆訓練進度"
      ],
      "metadata": {
        "id": "YKzNRP1KsEnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "def load_checkpoint_for_prediction(model, checkpoint_path, device):\n",
        "    \"\"\"\n",
        "    載入模型權重並設定為評估模式。修正了檢查點格式不一致的問題。\n",
        "    \"\"\"\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(f\"錯誤：找不到檢查點檔案於 {checkpoint_path}\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # 載入整個檢查點內容\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "        # 嘗試從標準的檢查點字典中讀取 'model_state_dict'\n",
        "        if 'model_state_dict' in checkpoint:\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            loaded_epoch = checkpoint.get('epoch', '未知')\n",
        "            print(f\"✅ 成功從標準檢查點 (Epoch {loaded_epoch}) 載入模型權重。\")\n",
        "        else:\n",
        "            # 假設整個檔案內容就是 model.state_dict()\n",
        "            model.load_state_dict(checkpoint)\n",
        "            print(\"✅ 成功從原始 State Dict 格式載入模型權重。\")\n",
        "\n",
        "        # 將模型切換到評估模式\n",
        "        model.eval()\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 載入權重失敗。錯誤訊息: {e}\")\n",
        "        return False\n",
        "\n",
        "# 📌 記得再次呼叫：\n",
        "# load_checkpoint_for_prediction(model, BEST_CHECKPOINT_PATH, DEVICE)\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "18jrsF80sOBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 預測與 NMS 實作\n",
        "> 要從模型輸出得到的邊界匡列表，需要兩個參數\n",
        "- decode_predictions\n",
        "- non_max_suppression\n",
        "\n"
      ],
      "metadata": {
        "id": "DhCfNKfzs5QJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 解碼函數"
      ],
      "metadata": {
        "id": "xW7zttEuuW0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def decode_predictions(predictions, anchors, img_size, conf_threshold=0.25):\n",
        "    \"\"\"\n",
        "    解碼 YOLO 模型輸出的原始張量，生成邊界框、信心度和類別分數。\n",
        "\n",
        "    :param predictions: 模型原始輸出 [B, A*6, G, G]\n",
        "    :param anchors: 該輸出層對應的錨框 (Normalized)\n",
        "    :param img_size: 輸入圖像尺寸 (例如 416)\n",
        "    :param conf_threshold: 物體信心度閾值\n",
        "    :return: 包含所有合格預測框的張量 [N_boxes, 6 (x1, y1, x2, y2, conf, cls)]\n",
        "    \"\"\"\n",
        "\n",
        "    # 設置參數\n",
        "    batch_size, _, grid_h, grid_w = predictions.shape\n",
        "    num_anchors = len(anchors)\n",
        "    num_classes = 1 # 只有 'pig' 一類\n",
        "\n",
        "    # 調整為 (Batch, Anchors, Grid_H, Grid_W, 6)\n",
        "    predictions = predictions.view(batch_size, num_anchors, num_classes + 5, grid_h, grid_w).permute(0, 1, 3, 4, 2).contiguous()\n",
        "\n",
        "    # 獲取原始 logits\n",
        "    pred_xy_logits = predictions[..., 0:2]\n",
        "    pred_wh_logits = predictions[..., 2:4]\n",
        "    pred_conf_logits = predictions[..., 4:5]\n",
        "    pred_cls_logits = predictions[..., 5:6]\n",
        "\n",
        "    # --- 1. 計算網格中心點座標 ---\n",
        "    grid_x = torch.arange(grid_w, device=predictions.device).repeat(grid_h, 1).view([1, 1, grid_h, grid_w]).type_as(predictions)\n",
        "    grid_y = torch.arange(grid_h, device=predictions.device).repeat(grid_w, 1).t().view([1, 1, grid_h, grid_w]).type_as(predictions)\n",
        "\n",
        "    # --- 2. 解碼座標與尺寸 ---\n",
        "\n",
        "    # a. x, y (中心點座標)\n",
        "    pred_x = (torch.sigmoid(pred_xy_logits[..., 0]) + grid_x) / grid_w\n",
        "    pred_y = (torch.sigmoid(pred_xy_logits[..., 1]) + grid_y) / grid_h\n",
        "\n",
        "    # b. w, h (尺寸)\n",
        "    anchors_wh = anchors.to(predictions.device).view(1, num_anchors, 1, 1, 2).type_as(predictions)\n",
        "    pred_w = torch.exp(pred_wh_logits[..., 0]) * anchors_wh[..., 0]\n",
        "    pred_h = torch.exp(pred_wh_logits[..., 1]) * anchors_wh[..., 1]\n",
        "\n",
        "    # --- 3. 轉換為 (x1, y1, x2, y2) 絕對座標 (Normalized 0-1) ---\n",
        "\n",
        "    # x1 = x_center - w/2\n",
        "    pred_x1 = pred_x - pred_w / 2\n",
        "    pred_y1 = pred_y - pred_h / 2\n",
        "    pred_x2 = pred_x + pred_w / 2\n",
        "    pred_y2 = pred_y + pred_h / 2\n",
        "\n",
        "    # --- 4. 計算最終分數 ---\n",
        "\n",
        "    # c. 信心度與分類\n",
        "    pred_conf = torch.sigmoid(pred_conf_logits.squeeze(-1)) # [B, A, G, G]\n",
        "    pred_cls = torch.sigmoid(pred_cls_logits.squeeze(-1))   # [B, A, G, G]\n",
        "\n",
        "    # 最終分數 = Conf * Class_Score (只有一類，這裡簡化)\n",
        "    # 由於只有一類，我們通常可以只看 pred_conf\n",
        "    final_score = pred_conf * pred_cls\n",
        "\n",
        "    # --- 5. 篩選與整合 ---\n",
        "\n",
        "    # 篩選掉低信心度的預測\n",
        "    mask = final_score > conf_threshold\n",
        "\n",
        "    # 獲取所有合格的邊界框數據\n",
        "\n",
        "    # 將所有 (B, A, G, G) 維度的數據展平\n",
        "    flat_x1 = pred_x1[mask]\n",
        "    flat_y1 = pred_y1[mask]\n",
        "    flat_x2 = pred_x2[mask]\n",
        "    flat_y2 = pred_y2[mask]\n",
        "    flat_scores = final_score[mask]\n",
        "\n",
        "    # 由於只有一類，類別 ID 永遠是 0\n",
        "    flat_cls = torch.zeros_like(flat_scores)\n",
        "\n",
        "    # 將所有數據合併為 [N_boxes, 6 (x1, y1, x2, y2, conf, cls)]\n",
        "    output = torch.stack([flat_x1, flat_y1, flat_x2, flat_y2, flat_scores, flat_cls], dim=1)\n",
        "\n",
        "    # 將 Normalized 座標 (0-1) 轉換為實際像素座標\n",
        "    # 這裡只轉換 x1, y1, x2, y2\n",
        "    if output.numel() > 0:\n",
        "        output[:, :4] *= img_size\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "MqMbHEGotQVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 非最大值抑制 (NMS)\n",
        "> 移除重疊的邊界框，並利用 Pytorch 提供的函式"
      ],
      "metadata": {
        "id": "nt-TtN6Gtyav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 需要安裝 torchvision\n",
        "# !pip install torchvision\n",
        "\n",
        "from torchvision.ops import nms\n",
        "\n",
        "def apply_nms(decoded_boxes, iou_threshold=0.45):\n",
        "    \"\"\"\n",
        "    對解碼後的邊界框應用非最大值抑制。\n",
        "\n",
        "    :param decoded_boxes: 從 decode_predictions 得到的張量 [N_boxes, 6 (x1, y1, x2, y2, conf, cls)]\n",
        "    :param iou_threshold: NMS 的 IoU 閾值\n",
        "    :return: NMS 後的最終邊界框張量\n",
        "    \"\"\"\n",
        "    if decoded_boxes.numel() == 0:\n",
        "        return decoded_boxes\n",
        "\n",
        "    # NMS 需要 (x1, y1, x2, y2) 格式的邊界框和信心度分數\n",
        "    boxes = decoded_boxes[:, :4] # [N, 4]\n",
        "    scores = decoded_boxes[:, 4] # [N]\n",
        "\n",
        "    # 執行 NMS\n",
        "    keep_indices = nms(boxes, scores, iou_threshold)\n",
        "\n",
        "    # 根據保留的索引返回最終的邊界框\n",
        "    final_boxes = decoded_boxes[keep_indices]\n",
        "    return final_boxes"
      ],
      "metadata": {
        "id": "SulfM8eHuAa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 產生提交檔 (.csv)"
      ],
      "metadata": {
        "id": "F8Jt9QYauuM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 呼叫載入函數"
      ],
      "metadata": {
        "id": "RM7Q-7rgu1qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 📌 替換為您保存的、驗證損失最低的 .pth 檔案路徑\n",
        "BEST_CHECKPOINT_PATH = '/content/drive/MyDrive/Colab Notebooks/電腦視覺與深度學習/taica-cvpdl-2025-hw-1/train/best_yolo_model (1).pth'\n",
        "\n",
        "# 呼叫 load_checkpoint_for_prediction 函數\n",
        "load_checkpoint_for_prediction(model, BEST_CHECKPOINT_PATH, DEVICE)\n",
        "\n",
        "# 確保模型已設置為評估模式\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "w5jB_6nPu4JG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2db54f-e5cb-4059-a041-94cc8884658d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 成功從原始 State Dict 格式載入模型權重。\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "YoloModel(\n",
              "  (backbone): SimpleBackbone(\n",
              "    (features): Sequential(\n",
              "      (0): ConvBlock(\n",
              "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
              "      )\n",
              "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (2): ConvBlock(\n",
              "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
              "      )\n",
              "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (4): ConvBlock(\n",
              "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
              "      )\n",
              "      (5): ConvBlock(\n",
              "        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
              "      )\n",
              "      (6): ConvBlock(\n",
              "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (head): YOLOHead(\n",
              "    (conv): Conv2d(128, 54, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 實現預測流程"
      ],
      "metadata": {
        "id": "t7fyfWxxvI95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 建立專門用於測試集的 `DataLoader`\n",
        "> 只需要負責加載圖片，不需要標註。"
      ],
      "metadata": {
        "id": "USf19eknwLef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        # 讀取所有圖片檔案名稱，並確保它們是 .jpg 格式\n",
        "        self.img_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 1. 取得圖片 ID\n",
        "        image_id = self.img_files[idx] # 檔案名稱，例如: '000000.jpg'\n",
        "\n",
        "        # 2. 載入圖片\n",
        "        img_path = os.path.join(self.img_dir, image_id)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # 3. 應用 Transform\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # 測試集只需返回圖片 Tensor 和 圖片 ID (檔案名稱)\n",
        "        return image, image_id"
      ],
      "metadata": {
        "id": "kbQtl9UQx3KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# 假設您的模型輸入尺寸是 416x416\n",
        "IMG_SIZE = 416\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    # 確保圖片調整到模型需要的輸入尺寸\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "\n",
        "    # 轉換為 Tensor\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # 正規化 (與訓練集的正規化參數保持一致)\n",
        "    # 這裡使用 ImageNet 的標準值作為範例，請根據您訓練時使用的值進行調整\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "z3xrq5QyyJhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ⚠️ 替換為您的測試集圖片實際路徑\n",
        "TEST_IMG_DIR = '/content/drive/MyDrive/Colab Notebooks/電腦視覺與深度學習/taica-cvpdl-2025-hw-1/test/img/'\n",
        "\n",
        "test_dataset = TestDataset(\n",
        "    img_dir=TEST_IMG_DIR,\n",
        "    transform=test_transforms\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=16,          # 預測時可以使用較大的 Batch Size\n",
        "    shuffle=False,          # 預測時必須是 False\n",
        "    num_workers=4           # 根據您的 Colab 配置調整\n",
        ")\n",
        "\n",
        "print(f\"測試集圖片總數: {len(test_dataset)}\")\n",
        "print(f\"測試集 DataLoader 創建成功。\")"
      ],
      "metadata": {
        "id": "e23bYymgyQxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1fd0738-92a1-44f5-e1c2-46a566f9591a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "測試集圖片總數: 1864\n",
            "測試集 DataLoader 創建成功。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 創建主預測迴圈\n",
        "> 主迴圈會跑過一遍測試集，執行解碼和 NMS，並將結果收集。"
      ],
      "metadata": {
        "id": "nc6f2ptOw-gB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def run_inference(model, test_loader, anchors, img_size, device):\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, image_ids in tqdm(test_loader, desc=\"進行測試集預測\"):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # 1. 模型前向傳播\n",
        "            predictions = model(images)\n",
        "\n",
        "            # 2. 解碼與 NMS (每個 Batch 處理)\n",
        "            for i in range(images.shape[0]):\n",
        "                single_image_output = predictions[i].unsqueeze(0) # 保持 Batch 維度為 1\n",
        "\n",
        "                # 解碼：將模型輸出轉換為邊界框\n",
        "                decoded_boxes = decode_predictions(\n",
        "                    single_image_output,\n",
        "                    anchors,\n",
        "                    img_size=img_size,\n",
        "                    conf_threshold=0.01 # 預測時閾值可以設低一點\n",
        "                )\n",
        "\n",
        "                # NMS：移除重疊框\n",
        "                final_boxes = apply_nms(decoded_boxes, iou_threshold=0.45)\n",
        "\n",
        "                # 3. 收集結果\n",
        "                if final_boxes.numel() > 0:\n",
        "                    # final_boxes: [N, 6] (x1, y1, x2, y2, conf, cls)\n",
        "\n",
        "                    # 轉換為 [x_min, y_min, w, h] 格式\n",
        "                    x1 = final_boxes[:, 0].cpu().numpy()\n",
        "                    y1 = final_boxes[:, 1].cpu().numpy()\n",
        "                    w = (final_boxes[:, 2] - final_boxes[:, 0]).cpu().numpy()\n",
        "                    h = (final_boxes[:, 3] - final_boxes[:, 1]).cpu().numpy()\n",
        "                    scores = final_boxes[:, 4].cpu().numpy()\n",
        "\n",
        "                    # 格式化為 RLE\n",
        "                    rle_string = format_to_rle(x1, y1, w, h, scores)\n",
        "\n",
        "                    all_predictions.append({\n",
        "                        'Image_ID': image_ids[i],\n",
        "                        'PredictionString': rle_string\n",
        "                    })\n",
        "                else:\n",
        "                    all_predictions.append({\n",
        "                        'Image_ID': image_ids[i],\n",
        "                        'PredictionString': '' # 沒有檢測到物體\n",
        "                    })\n",
        "\n",
        "    return pd.DataFrame(all_predictions)"
      ],
      "metadata": {
        "id": "2dUNbjarxNQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 實作 RLE 的格式化函數\n",
        "> 以符合作業要求上傳格式"
      ],
      "metadata": {
        "id": "eWfQRJnaxSFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_to_rle(x_min, y_min, w, h, scores):\n",
        "    \"\"\"\n",
        "    將邊界框數據轉換為 Kaggle 提交所需的最終格式字符串。\n",
        "    格式: [score x_min y_min w h class_id]\n",
        "    \"\"\"\n",
        "    if len(scores) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # 📌 根據範例，類別 ID 必須在最後，且為 0\n",
        "    # 這是單類別檢測常見的 ID\n",
        "    class_id = 0\n",
        "\n",
        "    rle_parts = []\n",
        "\n",
        "    # 將數據按照分數降序排列 (推薦做法)\n",
        "    sorted_indices = scores.argsort()[::-1]\n",
        "\n",
        "    for idx in sorted_indices:\n",
        "        # ⚠️ 最終順序修正為：score x_min y_min w h class_id\n",
        "        # 使用 .1f 確保座標精度符合範例風格\n",
        "        part = f\"{scores[idx]:.4f} {x_min[idx]:.1f} {y_min[idx]:.1f} {w[idx]:.1f} {h[idx]:.1f} {class_id}\"\n",
        "        rle_parts.append(part)\n",
        "\n",
        "    # 用空格連接所有片段\n",
        "    return \" \".join(rle_parts)"
      ],
      "metadata": {
        "id": "xInDRXgkxYKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 生成並下載結果檔案"
      ],
      "metadata": {
        "id": "xeIx_gvaxdI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 確保您已經成功載入權重並將模型設置為 eval 模式 (model.eval())\n",
        "\n",
        "# 呼叫預測函數，並將結果存入 final_df\n",
        "final_df = run_inference(\n",
        "    model=model,\n",
        "    test_loader=test_loader,\n",
        "    anchors=normalized_anchors,\n",
        "    img_size=IMG_SIZE,\n",
        "    device=DEVICE\n",
        ")\n",
        "\n",
        "print(\"✅ 預測結果 DataFrame (final_df) 已生成。\")\n",
        "print(f\"總共處理了 {len(final_df)} 張圖片。\")"
      ],
      "metadata": {
        "id": "o55vnNyv0dGQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4a3f013-756b-4cf5-c481-50b813293b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r進行測試集預測:   0%|          | 0/117 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7e7e7e9ad4e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7e7e7e9ad4e0>^^\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    ^self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "^    if w.is_alive():\n",
            " ^  ^ ^  \n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "     ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^ ^ ^ ^ ^  ^ \n",
            "^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "^^    assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^ ^ ^ ^ ^^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^AssertionError^: ^can only test a child process^\n",
            "^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7e7e7e9ad4e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            " Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7e7e7e9ad4e0> \n",
            "Traceback (most recent call last):\n",
            "   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers() \n",
            "   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "  ^    if w.is_alive():^^\n",
            " ^ ^ ^ ^^ ^^^ ^ \n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "^^   ^ ^ ^  ^^ ^  ^^ \n",
            "^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^ ^ ^^  ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^AssertionError^: ^can only test a child process^\n",
            "^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "進行測試集預測: 100%|██████████| 117/117 [01:17<00:00,  1.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 預測結果 DataFrame (final_df) 已生成。\n",
            "總共處理了 1864 張圖片。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 假設 final_df 已經從 run_inference 函數中獲得\n",
        "final_df.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"提交檔案 submission.csv 已生成！\")\n",
        "# 您可以在 Colab 左側的文件瀏覽器中下載此檔案並上傳到 Kaggle。"
      ],
      "metadata": {
        "id": "mhAK8b7pxhJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e03d018e-5d82-4d2a-c0cc-7ac7ad79c731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "提交檔案 submission.csv 已生成！\n"
          ]
        }
      ]
    }
  ]
}